
"""
ml10.py â€” A tiny, self-contained Python module implementing 10 classic ML algorithms
with a UNIFIED request/response interface suitable for project services.

Algorithms included (10 total):
1) DecisionTreeClassifier (CART, Gini)
2) LinearSVMClassifier (multi-class OVR, SGD-optimized hinge loss)
3) KNNClassifier
4) GaussianNaiveBayes
5) LogisticRegressionClassifier (multi-class OVR)
6) MLPClassifier (1-hidden-layer, softmax)
7) LinearRegressionModel (OLS with optional L2)
8) MLPRegressor (1-hidden-layer, MSE)
9) KMeansClustering
10) PCA (via SVD; predict() == transform())

Unified API (stateless):
- serve_request(payload: dict) -> dict
  payload = {
    "algo": str,                    # one of the keys in ALGORITHMS below
    "action": "train"|"infer"|"score",
    "X": list[list[float]],         # features (n_samples x n_features)
    "y": list | None,               # targets/labels when needed
    "params": dict | None,          # algo hyperparams
    "state": dict | None            # previously returned model state for "infer"/"score"
  }

Return:
- {
    "ok": bool,
    "algo": str,
    "task_type": "classification"|"regression"|"unsupervised"|"dimensionality_reduction",
    "action": str,
    "y_pred": list | None,          # predictions (labels / values). For PCA: transformed features as list[list]
    "y_proba": list[list] | None,   # for classifiers that can output probabilities (if available)
    "metrics": dict | None,         # accuracy / MAE / MSE / inertia etc.
    "state": dict | None,           # serializable model parameters
    "message": str | None
  }

Notes:
- No third-party ML libs are used; only numpy.
- All states (weights, thresholds, prototypes) are JSON-serializable.
- This module is intentionally compact and educational; not optimized for big data.
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple
import math
import random
import numpy as np

# ------------ Utilities ------------

def to_numpy(X):
    X = np.asarray(X, dtype=float)
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    return X

def to_numpy_1d(y):
    if y is None:
        return None
    y = np.asarray(y)
    return y.astype(int) if y.dtype.kind in "iu" else y.astype(float)

def one_hot(y: np.ndarray, num_classes: int) -> np.ndarray:
    Y = np.zeros((y.shape[0], num_classes), dtype=float)
    Y[np.arange(y.shape[0]), y] = 1.0
    return Y

def softmax(z: np.ndarray) -> np.ndarray:
    z = z - np.max(z, axis=1, keepdims=True)
    e = np.exp(z)
    return e / (np.sum(e, axis=1, keepdims=True) + 1e-12)

def relu(x): return np.maximum(0.0, x)
def relu_grad(x): return (x > 0).astype(float)

def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))
def sigmoid_grad(z): 
    s = sigmoid(z); 
    return s * (1.0 - s)

# ---- Metrics ----

def accuracy(y_true, y_pred) -> float:
    y_true = to_numpy_1d(y_true).ravel().astype(int)
    y_pred = to_numpy_1d(y_pred).ravel().astype(int)
    return float(np.mean(y_true == y_pred)) if len(y_true) else 0.0

def mse(y_true, y_pred) -> float:
    y_true = to_numpy_1d(y_true).ravel().astype(float)
    y_pred = to_numpy_1d(y_pred).ravel().astype(float)
    return float(np.mean((y_true - y_pred) ** 2)) if len(y_true) else 0.0

def mae(y_true, y_pred) -> float:
    y_true = to_numpy_1d(y_true).ravel().astype(float)
    y_pred = to_numpy_1d(y_pred).ravel().astype(float)
    return float(np.mean(np.abs(y_true - y_pred))) if len(y_true) else 0.0

# ------------ Base Class ------------

class BaseModel:
    task_type: str = "base"
    name: str = "base"
    def fit(self, X, y=None): raise NotImplementedError
    def predict(self, X): raise NotImplementedError
    def predict_proba(self, X): return None
    def get_state(self) -> Dict[str, Any]: return {}
    def set_state(self, state: Dict[str, Any]): return self
    def get_params(self) -> Dict[str, Any]: return {}
    def set_params(self, **params): return self

# ------------ 1) Decision Tree (CART) Classifier ------------

class DecisionTreeClassifier(BaseModel):
    task_type = "classification"
    name = "decision_tree_clf"
    def __init__(self, max_depth: int = 10, min_samples_split: int = 2, n_features: Optional[int] = None):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.n_features = n_features  # if None use all
        self.tree_ = None

    def _gini(self, y):
        m = len(y)
        if m == 0: return 0.0
        _, counts = np.unique(y, return_counts=True)
        p = counts / m
        return 1.0 - np.sum(p**2)

    def _best_split(self, X, y):
        m, n = X.shape
        features = range(n) if self.n_features is None else np.random.choice(n, self.n_features, replace=False)
        best = {"gini": 1.0, "idx": None, "thr": None}
        for idx in features:
            thresholds = np.unique(X[:, idx])
            for thr in thresholds:
                left = y[X[:, idx] <= thr]
                right = y[X[:, idx] > thr]
                g = (len(left)*self._gini(left) + len(right)*self._gini(right)) / m
                if g < best["gini"]:
                    best = {"gini": g, "idx": int(idx), "thr": float(thr)}
        return best["idx"], best["thr"]

    def _build(self, X, y, depth=0):
        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]
        predicted_class = int(np.argmax(num_samples_per_class))
        node = {"type":"leaf", "class": predicted_class}

        if depth < self.max_depth and len(y) >= self.min_samples_split and self._gini(y) > 0.0:
            idx, thr = self._best_split(X, y)
            if idx is not None:
                left_idx = X[:, idx] <= thr
                node = {
                    "type":"node",
                    "idx": int(idx),
                    "thr": float(thr),
                    "left": self._build(X[left_idx], y[left_idx], depth+1),
                    "right": self._build(X[~left_idx], y[~left_idx], depth+1)
                }
        return node

    def fit(self, X, y):
        X, y = to_numpy(X), to_numpy_1d(y).astype(int)
        self.tree_ = self._build(X, y, depth=0)
        return self

    def _predict_one(self, x, node):
        if node["type"] == "leaf":
            return node["class"]
        return self._predict_one(x, node["left"] if x[node["idx"]] <= node["thr"] else node["right"])

    def predict(self, X):
        X = to_numpy(X)
        return np.array([self._predict_one(x, self.tree_) for x in X], dtype=int)

    def get_state(self): 
        return {"max_depth": self.max_depth, "min_samples_split": self.min_samples_split,
                "n_features": self.n_features, "tree": self.tree_}

    def set_state(self, state):
        self.max_depth = state.get("max_depth", 10)
        self.min_samples_split = state.get("min_samples_split", 2)
        self.n_features = state.get("n_features", None)
        self.tree_ = state.get("tree", None)
        return self

# ------------ 2) Linear SVM Classifier (OVR, SGD hinge) ------------

class LinearSVMClassifier(BaseModel):
    task_type = "classification"
    name = "svm_clf"
    def __init__(self, lr: float = 1e-2, epochs: int = 200, C: float = 1.0, random_state: int = 42):
        self.lr = lr
        self.epochs = epochs
        self.C = C
        self.random_state = random_state
        self.W = None  # (n_features, n_classes)
        self.b = None

    def fit(self, X, y):
        X, y = to_numpy(X), to_numpy_1d(y).astype(int)
        n, d = X.shape
        classes = np.unique(y)
        k = len(classes)
        rng = np.random.default_rng(self.random_state)
        self.W = np.zeros((d, k))
        self.b = np.zeros(k)
        for epoch in range(self.epochs):
            idx = rng.permutation(n)
            for i in idx:
                xi, yi = X[i], y[i]
                scores = xi @ self.W + self.b
                margins = 1 - scores + scores[yi]
                margins[yi] = 0.0
                j = np.argmax(margins)
                # if margin violated for class j
                if 1 - (scores[yi] - scores[j]) > 0:
                    # gradient update
                    self.W[:, yi] += self.lr * (self.C * xi)
                    self.b[yi] += self.lr * self.C
                    self.W[:, j] -= self.lr * (self.C * xi)
                    self.b[j] -= self.lr * self.C
                # regularize (L2)
                self.W *= (1 - self.lr)
        return self

    def predict(self, X):
        X = to_numpy(X)
        scores = X @ self.W + self.b
        return np.argmax(scores, axis=1)

    def get_state(self): 
        return {"lr": self.lr, "epochs": self.epochs, "C": self.C, "random_state": self.random_state,
                "W": self.W.tolist() if self.W is not None else None,
                "b": self.b.tolist() if self.b is not None else None}

    def set_state(self, state):
        self.lr = state.get("lr", 1e-2); self.epochs = state.get("epochs", 200)
        self.C = state.get("C", 1.0); self.random_state = state.get("random_state", 42)
        self.W = np.array(state.get("W")) if state.get("W") is not None else None
        self.b = np.array(state.get("b")) if state.get("b") is not None else None
        return self

# ------------ 3) KNN Classifier ------------

class KNNClassifier(BaseModel):
    task_type = "classification"
    name = "knn_clf"
    def __init__(self, k: int = 5, weighted: bool = False):
        self.k = k
        self.weighted = weighted
        self.X = None
        self.y = None

    def fit(self, X, y):
        self.X = to_numpy(X)
        self.y = to_numpy_1d(y).astype(int)
        return self

    def predict(self, X):
        X = to_numpy(X)
        preds = []
        for xi in X:
            dists = np.linalg.norm(self.X - xi, axis=1)
            idx = np.argsort(dists)[:self.k]
            votes = self.y[idx]
            if self.weighted:
                w = 1.0 / (dists[idx] + 1e-9)
                scores = {}
                for v, ww in zip(votes, w):
                    scores[v] = scores.get(v, 0.0) + ww
                preds.append(max(scores.items(), key=lambda kv: kv[1])[0])
            else:
                preds.append(np.bincount(votes).argmax())
        return np.array(preds, dtype=int)

    def get_state(self):
        return {"k": self.k, "weighted": self.weighted,
                "X": self.X.tolist() if self.X is not None else None,
                "y": self.y.tolist() if self.y is not None else None}

    def set_state(self, state):
        self.k = state.get("k", 5); self.weighted = state.get("weighted", False)
        self.X = np.array(state.get("X")) if state.get("X") is not None else None
        self.y = np.array(state.get("y")) if state.get("y") is not None else None
        return self

# ------------ 4) Gaussian Naive Bayes ------------

class GaussianNaiveBayes(BaseModel):
    task_type = "classification"
    name = "gaussian_nb"
    def __init__(self, eps: float = 1e-9):
        self.eps = eps
        self.classes_ = None
        self.prior_ = None
        self.mean_ = None
        self.var_ = None

    def fit(self, X, y):
        X, y = to_numpy(X), to_numpy_1d(y).astype(int)
        self.classes_ = np.unique(y)
        k = len(self.classes_)
        n_features = X.shape[1]
        self.mean_ = np.zeros((k, n_features))
        self.var_ = np.zeros((k, n_features))
        self.prior_ = np.zeros(k)
        for i, c in enumerate(self.classes_):
            Xc = X[y == c]
            self.mean_[i] = Xc.mean(axis=0)
            self.var_[i] = Xc.var(axis=0) + self.eps
            self.prior_[i] = Xc.shape[0] / X.shape[0]
        return self

    def _log_gauss(self, i, X):
        mean, var = self.mean_[i], self.var_[i]
        return -0.5 * np.sum(np.log(2*np.pi*var) + ((X - mean)**2)/var, axis=1)

    def predict_proba(self, X):
        X = to_numpy(X)
        log_probs = []
        for i in range(len(self.classes_)):
            log_probs.append(self._log_gauss(i, X) + np.log(self.prior_[i] + self.eps))
        log_probs = np.vstack(log_probs).T
        # normalize
        log_probs -= log_probs.max(axis=1, keepdims=True)
        probs = np.exp(log_probs)
        probs /= probs.sum(axis=1, keepdims=True)
        return probs

    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)

    def get_state(self):
        return {"eps": self.eps,
                "classes": self.classes_.tolist() if self.classes_ is not None else None,
                "prior": self.prior_.tolist() if self.prior_ is not None else None,
                "mean": self.mean_.tolist() if self.mean_ is not None else None,
                "var": self.var_.tolist() if self.var_ is not None else None}

    def set_state(self, state):
        self.eps = state.get("eps", 1e-9)
        self.classes_ = np.array(state.get("classes")) if state.get("classes") is not None else None
        self.prior_ = np.array(state.get("prior")) if state.get("prior") is not None else None
        self.mean_ = np.array(state.get("mean")) if state.get("mean") is not None else None
        self.var_ = np.array(state.get("var")) if state.get("var") is not None else None
        return self

# ------------ 5) Logistic Regression (OVR) ------------

class LogisticRegressionClassifier(BaseModel):
    task_type = "classification"
    name = "logreg_clf"
    def __init__(self, lr: float = 0.1, epochs: int = 200, l2: float = 0.0):
        self.lr = lr; self.epochs = epochs; self.l2 = l2
        self.W = None; self.b = None

    def fit(self, X, y):
        X, y = to_numpy(X), to_numpy_1d(y).astype(int)
        n, d = X.shape; classes = np.unique(y); k = len(classes)
        self.W = np.zeros((d, k)); self.b = np.zeros(k)
        Y = one_hot(y, k)
        for _ in range(self.epochs):
            logits = X @ self.W + self.b
            P = softmax(logits)
            grad_W = X.T @ (P - Y) / n + self.l2 * self.W
            grad_b = np.mean(P - Y, axis=0)
            self.W -= self.lr * grad_W
            self.b -= self.lr * grad_b
        return self

    def predict_proba(self, X):
        X = to_numpy(X)
        return softmax(X @ self.W + self.b)

    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)

    def get_state(self):
        return {"lr": self.lr, "epochs": self.epochs, "l2": self.l2,
                "W": self.W.tolist() if self.W is not None else None,
                "b": self.b.tolist() if self.b is not None else None}

    def set_state(self, state):
        self.lr = state.get("lr", 0.1); self.epochs = state.get("epochs", 200); self.l2 = state.get("l2", 0.0)
        self.W = np.array(state.get("W")) if state.get("W") is not None else None
        self.b = np.array(state.get("b")) if state.get("b") is not None else None
        return self

# ------------ 6) MLP Classifier ------------

class MLPClassifier(BaseModel):
    task_type = "classification"
    name = "mlp_clf"
    def __init__(self, hidden: int = 64, lr: float = 0.1, epochs: int = 200, batch_size: int = 32, random_state: int = 42):
        self.hidden = hidden; self.lr = lr; self.epochs = epochs; self.batch_size = batch_size
        self.random_state = random_state
        self.W1 = None; self.b1 = None; self.W2 = None; self.b2 = None

    def fit(self, X, y):
        X, y = to_numpy(X), to_numpy_1d(y).astype(int)
        n, d = X.shape; k = len(np.unique(y))
        rng = np.random.default_rng(self.random_state)
        self.W1 = rng.normal(scale=0.01, size=(d, self.hidden)); self.b1 = np.zeros(self.hidden)
        self.W2 = rng.normal(scale=0.01, size=(self.hidden, k)); self.b2 = np.zeros(k)
        Y = one_hot(y, k)
        for _ in range(self.epochs):
            # mini-batch
            idx = rng.permutation(n)
            for i in range(0, n, self.batch_size):
                batch = idx[i:i+self.batch_size]
                Xb, Yb = X[batch], Y[batch]
                h_pre = Xb @ self.W1 + self.b1
                h = relu(h_pre)
                logits = h @ self.W2 + self.b2
                P = softmax(logits)
                # backprop
                dlogits = (P - Yb) / Xb.shape[0]
                dW2 = h.T @ dlogits
                db2 = dlogits.mean(axis=0) * Xb.shape[0]
                dh = dlogits @ self.W2.T
                dh_pre = dh * relu_grad(h_pre)
                dW1 = Xb.T @ dh_pre
                db1 = dh_pre.mean(axis=0) * Xb.shape[0]
                # update
                self.W2 -= self.lr * dW2; self.b2 -= self.lr * db2
                self.W1 -= self.lr * dW1; self.b1 -= self.lr * db1
        return self

    def predict_proba(self, X):
        X = to_numpy(X)
        h = relu(X @ self.W1 + self.b1)
        return softmax(h @ self.W2 + self.b2)

    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)

    def get_state(self):
        s = {"hidden": self.hidden, "lr": self.lr, "epochs": self.epochs, "batch_size": self.batch_size,
             "random_state": self.random_state,
             "W1": self.W1.tolist() if self.W1 is not None else None,
             "b1": self.b1.tolist() if self.b1 is not None else None,
             "W2": self.W2.tolist() if self.W2 is not None else None,
             "b2": self.b2.tolist() if self.b2 is not None else None}
        return s

    def set_state(self, state):
        self.hidden = state.get("hidden", 64); self.lr = state.get("lr", 0.1)
        self.epochs = state.get("epochs", 200); self.batch_size = state.get("batch_size", 32)
        self.random_state = state.get("random_state", 42)
        self.W1 = np.array(state.get("W1")) if state.get("W1") is not None else None
        self.b1 = np.array(state.get("b1")) if state.get("b1") is not None else None
        self.W2 = np.array(state.get("W2")) if state.get("W2") is not None else None
        self.b2 = np.array(state.get("b2")) if state.get("b2") is not None else None
        return self

# ------------ 7) Linear Regression (OLS + optional Ridge) ------------

class LinearRegressionModel(BaseModel):
    task_type = "regression"
    name = "linear_reg"
    def __init__(self, l2: float = 0.0):
        self.l2 = l2
        self.W = None  # (d,)
        self.b = 0.0

    def fit(self, X, y):
        X = to_numpy(X); y = to_numpy_1d(y).astype(float)
        n, d = X.shape
        X_ext = np.hstack([X, np.ones((n,1))])
        A = X_ext.T @ X_ext
        if self.l2 > 0:
            reg = self.l2 * np.eye(d+1)
            reg[-1, -1] = 0.0  # don't regularize bias
            A = A + reg
        w_ext = np.linalg.pinv(A) @ (X_ext.T @ y)
        self.W = w_ext[:-1]; self.b = w_ext[-1]
        return self

    def predict(self, X):
        X = to_numpy(X)
        return X @ self.W + self.b

    def get_state(self):
        return {"l2": self.l2, "W": self.W.tolist() if self.W is not None else None, "b": float(self.b)}

    def set_state(self, state):
        self.l2 = state.get("l2", 0.0)
        self.W = np.array(state.get("W")) if state.get("W") is not None else None
        self.b = float(state.get("b", 0.0))
        return self

# ------------ 8) MLP Regressor ------------

class MLPRegressor(BaseModel):
    task_type = "regression"
    name = "mlp_reg"
    def __init__(self, hidden: int = 64, lr: float = 0.01, epochs: int = 300, batch_size: int = 32, random_state: int = 42):
        self.hidden = hidden; self.lr = lr; self.epochs = epochs; self.batch_size = batch_size
        self.random_state = random_state
        self.W1 = None; self.b1 = None; self.W2 = None; self.b2 = None

    def fit(self, X, y):
        X = to_numpy(X); y = to_numpy_1d(y).reshape(-1,1).astype(float)
        n, d = X.shape
        rng = np.random.default_rng(self.random_state)
        self.W1 = rng.normal(scale=0.01, size=(d, self.hidden)); self.b1 = np.zeros(self.hidden)
        self.W2 = rng.normal(scale=0.01, size=(self.hidden, 1)); self.b2 = np.zeros(1)
        for _ in range(self.epochs):
            idx = rng.permutation(n)
            for i in range(0, n, self.batch_size):
                b = idx[i:i+self.batch_size]
                Xb, yb = X[b], y[b]
                h_pre = Xb @ self.W1 + self.b1
                h = relu(h_pre)
                yhat = h @ self.W2 + self.b2
                err = (yhat - yb)
                dW2 = h.T @ err / Xb.shape[0]
                db2 = err.mean(axis=0) * Xb.shape[0]
                dh = err @ self.W2.T
                dh_pre = dh * relu_grad(h_pre)
                dW1 = Xb.T @ dh_pre / Xb.shape[0]
                db1 = dh_pre.mean(axis=0) * Xb.shape[0]
                self.W2 -= self.lr * dW2; self.b2 -= self.lr * db2
                self.W1 -= self.lr * dW1; self.b1 -= self.lr * db1
        return self

    def predict(self, X):
        X = to_numpy(X)
        h = relu(X @ self.W1 + self.b1)
        return (h @ self.W2 + self.b2).ravel()

    def get_state(self):
        return {"hidden": self.hidden, "lr": self.lr, "epochs": self.epochs, "batch_size": self.batch_size,
                "random_state": self.random_state,
                "W1": self.W1.tolist() if self.W1 is not None else None,
                "b1": self.b1.tolist() if self.b1 is not None else None,
                "W2": self.W2.tolist() if self.W2 is not None else None,
                "b2": self.b2.tolist() if self.b2 is not None else None}

    def set_state(self, state):
        self.hidden = state.get("hidden", 64); self.lr = state.get("lr", 0.01)
        self.epochs = state.get("epochs", 300); self.batch_size = state.get("batch_size", 32)
        self.random_state = state.get("random_state", 42)
        self.W1 = np.array(state.get("W1")) if state.get("W1") is not None else None
        self.b1 = np.array(state.get("b1")) if state.get("b1") is not None else None
        self.W2 = np.array(state.get("W2")) if state.get("W2") is not None else None
        self.b2 = np.array(state.get("b2")) if state.get("b2") is not None else None
        return self

# ------------ 9) K-Means Clustering ------------

class KMeansClustering(BaseModel):
    task_type = "unsupervised"
    name = "kmeans"
    def __init__(self, k: int = 3, max_iter: int = 200, tol: float = 1e-4, random_state: int = 42):
        self.k = k; self.max_iter = max_iter; self.tol = tol; self.random_state = random_state
        self.centers_ = None; self.inertia_ = None

    def fit(self, X, y=None):
        X = to_numpy(X)
        n, d = X.shape
        rng = np.random.default_rng(self.random_state)
        idx = rng.choice(n, self.k, replace=False)
        centers = X[idx].copy()
        for _ in range(self.max_iter):
            # assign
            dists = np.linalg.norm(X[:, None, :] - centers[None, :, :], axis=2)
            labels = np.argmin(dists, axis=1)
            new_centers = np.array([X[labels == i].mean(axis=0) if np.any(labels == i) else centers[i] for i in range(self.k)])
            shift = np.linalg.norm(new_centers - centers)
            centers = new_centers
            if shift < self.tol: break
        self.centers_ = centers
        self.inertia_ = float(((X - centers[labels])**2).sum())
        return self

    def predict(self, X):
        X = to_numpy(X)
        dists = np.linalg.norm(X[:, None, :] - self.centers_[None, :, :], axis=2)
        return np.argmin(dists, axis=1)

    def get_state(self):
        return {"k": self.k, "max_iter": self.max_iter, "tol": self.tol, "random_state": self.random_state,
                "centers": self.centers_.tolist() if self.centers_ is not None else None,
                "inertia": self.inertia_}

    def set_state(self, state):
        self.k = state.get("k", 3); self.max_iter = state.get("max_iter", 200)
        self.tol = state.get("tol", 1e-4); self.random_state = state.get("random_state", 42)
        self.centers_ = np.array(state.get("centers")) if state.get("centers") is not None else None
        self.inertia_ = state.get("inertia")
        return self

# ------------ 10) PCA ------------

class PCA(BaseModel):
    task_type = "dimensionality_reduction"
    name = "pca"
    def __init__(self, n_components: int = 2, whiten: bool = False):
        self.n_components = n_components; self.whiten = whiten
        self.mean_ = None; self.components_ = None; self.var_ = None

    def fit(self, X, y=None):
        X = to_numpy(X)
        self.mean_ = X.mean(axis=0)
        Xc = X - self.mean_
        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)
        self.components_ = Vt[:self.n_components]
        self.var_ = (S**2) / (X.shape[0]-1)
        return self

    def transform(self, X):
        X = to_numpy(X) - self.mean_
        Z = X @ self.components_.T
        if self.whiten:
            comp_var = self.var_[:self.n_components]
            Z = Z / np.sqrt(comp_var + 1e-9)
        return Z

    def predict(self, X):
        # for unified API, predict() returns transformed features
        return self.transform(X)

    def get_state(self):
        return {"n_components": self.n_components, "whiten": self.whiten,
                "mean": self.mean_.tolist() if self.mean_ is not None else None,
                "components": self.components_.tolist() if self.components_ is not None else None,
                "var": self.var_.tolist() if self.var_ is not None else None}

    def set_state(self, state):
        self.n_components = state.get("n_components", 2); self.whiten = state.get("whiten", False)
        self.mean_ = np.array(state.get("mean")) if state.get("mean") is not None else None
        self.components_ = np.array(state.get("components")) if state.get("components") is not None else None
        self.var_ = np.array(state.get("var")) if state.get("var") is not None else None
        return self

# ------------ Registry, factory & service ------------

ALGORITHMS = {
    # classification
    "decision_tree_clf": DecisionTreeClassifier,
    "svm_clf": LinearSVMClassifier,
    "knn_clf": KNNClassifier,
    "gaussian_nb": GaussianNaiveBayes,
    "logreg_clf": LogisticRegressionClassifier,
    "mlp_clf": MLPClassifier,
    # regression
    "linear_reg": LinearRegressionModel,
    "mlp_reg": MLPRegressor,
    # unsupervised
    "kmeans": KMeansClustering,
    "pca": PCA,
}

def _build_model(algo: str, params: Optional[Dict[str, Any]] = None, state: Optional[Dict[str, Any]] = None) -> BaseModel:
    if algo not in ALGORITHMS:
        raise ValueError(f"Unknown algo '{algo}'. Valid: {list(ALGORITHMS.keys())}")
    cls = ALGORITHMS[algo]
    model = cls(**(params or {}))
    if state:
        model.set_state(state)
    return model

def serve_request(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Stateless service entry. Train / Infer / Score with a unified JSON payload.
    """
    try:
        algo = payload["algo"]
        action = payload["action"]
        X = payload.get("X", None)
        y = payload.get("y", None)
        params = payload.get("params", None)
        state = payload.get("state", None)

        if X is None:
            return {"ok": False, "algo": algo, "action": action, "message": "X is required"}

        model = _build_model(algo, params, state)
        task_type = getattr(model, "task_type", "unknown")

        y_pred = None; y_proba = None; metrics = None; new_state = None

        if action == "train":
            model.fit(X, y)
            # after training, we can report quick training metrics if y exists
            if task_type == "classification" and y is not None:
                y_pred = model.predict(X).tolist()
                metrics = {"accuracy": accuracy(y, y_pred)}
            elif task_type == "regression" and y is not None:
                y_pred = model.predict(X).tolist()
                metrics = {"mse": mse(y, y_pred), "mae": mae(y, y_pred)}
            elif task_type == "unsupervised":
                if hasattr(model, "inertia_"):
                    metrics = {"inertia": getattr(model, "inertia_")}
            new_state = model.get_state()

        elif action == "infer":
            # require state (model previously trained)
            if state is None:
                return {"ok": False, "algo": algo, "action": action, "message": "state is required for infer"}
            y_pred = model.predict(X)
            if isinstance(y_pred, np.ndarray) and y_pred.ndim == 2:
                y_pred = y_pred.tolist()  # e.g., PCA transform
            else:
                y_pred = to_numpy_1d(y_pred).tolist()
            try:
                proba = model.predict_proba(X)
                if proba is not None:
                    y_proba = proba.tolist()
            except Exception:
                y_proba = None

        elif action == "score":
            if y is None:
                return {"ok": False, "algo": algo, "action": action, "message": "y is required for score"}
            if state is None:
                return {"ok": False, "algo": algo, "action": action, "message": "state is required for score"}
            y_hat = _build_model(algo, params, state).predict(X)
            if task_type == "classification":
                metrics = {"accuracy": accuracy(y, y_hat)}
            elif task_type == "regression":
                metrics = {"mse": mse(y, y_hat), "mae": mae(y, y_hat)}
            else:
                metrics = {}
        else:
            return {"ok": False, "algo": algo, "action": action, "message": f"Unknown action {action}"}

        return {"ok": True, "algo": algo, "task_type": task_type, "action": action,
                "y_pred": y_pred, "y_proba": y_proba, "metrics": metrics, "state": new_state, "message": None}

    except Exception as e:
        return {"ok": False, "algo": payload.get("algo"), "action": payload.get("action"),
                "message": f"{type(e).__name__}: {str(e)}"}

# -------------- Tiny self-test (optional) --------------
if __name__ == "__main__":
    # A quick smoke test on a toy dataset (2 classes)
    X = [[0,0],[0,1],[1,0],[1,1],[2,2],[2,3],[3,2],[3,3]]
    y = [0,0,0,0,1,1,1,1]
    req = {"algo": "logreg_clf", "action": "train", "X": X, "y": y, "params": {"epochs": 300, "lr": 0.2}}
    out = serve_request(req)
    print("Train:", out["metrics"])
    state = out["state"]
    inf = serve_request({"algo":"logreg_clf","action":"infer","X":[[1.5,1.5],[0.2,0.1]],"state":state})
    print("Infer:", inf["y_pred"])
